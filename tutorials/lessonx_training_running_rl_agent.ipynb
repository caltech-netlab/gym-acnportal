{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is strongly suggested that this tutorial is run in its own environment (e.g. conda or pyenv), as it will require dependencies not required by the rest of ACN-Portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running in a new environment, such as Google Colab, run this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/zach401/acnportal.git\n",
    "# !pip install acnportal/.[gym]\n",
    "# !pip install stable-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACN-Sim Tutorial: Lesson X\n",
    "## Training and Running a Reinforcement Learning Agent on ACN-Sim\n",
    "### by Sunash Sharma\n",
    "#### Last updated: 02/26/2020\n",
    "\n",
    "In this lesson we will learn how to train a reinforcement learning (RL) agent and run it using OpenAI Gym environments that wrap ACN-Sim. For this example we will be using the stable-baselines proximal policy optimization (PPO2) algorithm. As such, running this tutorial requires the stable-baselines package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson, we will use a simple example. Imagine we have a single charger, with EV's plugging in over a set duration. Each EV has a random arrival and departure time, requesting an amount of energy that equates to a laxity of $d/2$, where $d$ is staying duration. (i.e. we may charge at half the maximum rate for the entire staying time and deliver all the energy requested). First, let's make some functions to generate Simulation instances that simulate this scenario. We'll start by defining a function which generates random plugins for a single EVSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from typing import List, Callable, Optional, Dict, Any\n",
    "import numpy as np\n",
    "import gym\n",
    "import pytz\n",
    "from gym.wrappers import FlattenObservation\n",
    "from gym_acnportal import GymTrainedInterface, GymTrainingInterface\n",
    "from matplotlib import pyplot as plt\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common import BaseRLModel\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "from acnportal import acnsim\n",
    "from acnportal.acnsim import events, models, Simulator\n",
    "\n",
    "from gym_acnportal.algorithms import SimRLModelWrapper\n",
    "from gym_acnportal.gym_acnsim.envs.action_spaces import SimAction\n",
    "from gym_acnportal.gym_acnsim.envs import (\n",
    "    BaseSimEnv,\n",
    "    reward_functions,\n",
    "    CustomSimEnv,\n",
    "    default_action_object,\n",
    "    default_observation_objects,\n",
    ")\n",
    "from gym_acnportal.gym_acnsim.envs.observation import SimObservation\n",
    "from acnportal.algorithms import BaseAlgorithm, Interface\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "def random_plugin(num, time_limit, evse, laxity_ratio=1/2,\n",
    "                  max_rate=32, voltage=208, period=1):\n",
    "    \"\"\" Returns a list of num random plugin events occurring anytime from time 0\n",
    "    to time_limit. Each plugin has a random arrival and departure under the time\n",
    "    limit, and a satisfiable requested energy assuming no other cars plugged in.\n",
    "    Each EV has initial laxity equal to half the staying duration unless\n",
    "    otherwise specified.\n",
    "    \n",
    "    The plugins occur for a single EVSE, whose maximal rate and voltage are\n",
    "    assumed to be 32 A and  208 V, respectively, unless otherwise specified.\n",
    "    \n",
    "    Args: TODO\n",
    "    Return: TODO\n",
    "    \"\"\"\n",
    "    out_event_lst = [None] * num\n",
    "    times = []\n",
    "    i = 0\n",
    "    while i < 2 * num:\n",
    "        rnum = random.randint(0, time_limit)\n",
    "        if rnum not in times:\n",
    "            times.append(rnum)\n",
    "            i += 1\n",
    "    times = sorted(times)\n",
    "    battery = models.Battery(100, 0, 100)\n",
    "    for i in range(num):\n",
    "        arrival_time = times[2*i]\n",
    "        departure_time = times[2*i+1]\n",
    "        requested_energy = (\n",
    "            (departure_time - arrival_time) / (60 / period) \n",
    "            * max_rate * voltage / (1 / laxity_ratio)\n",
    "        )\n",
    "        ev = models.EV(arrival_time, departure_time, requested_energy,\n",
    "                       evse, f'rs-{evse}-{i}', battery)\n",
    "        out_event_lst[i] = events.PluginEvent(arrival_time, ev)\n",
    "    return out_event_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above event generation is stochastic, we'll want to completely rebuild the simulation each time the environment is reset, so that the next simulation has a new event queue. As such, we will define a simulation generating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def _random_sim_builder(\n",
    "    algorithm: Optional[BaseAlgorithm], interface_type: type\n",
    ") -> Simulator:\n",
    "    timezone = pytz.timezone(\"America/Los_Angeles\")\n",
    "    start = timezone.localize(datetime(2018, 9, 5))\n",
    "    period = 1\n",
    "\n",
    "    # Make random event queue\n",
    "    cn = acnsim.sites.simple_acn([\"EVSE-001\"], aggregate_cap=20 * 208 / 1000)\n",
    "    event_list = []\n",
    "    for station_id in cn.station_ids:\n",
    "        event_list.extend(random_plugin(10, 100, station_id))\n",
    "    event_queue = events.EventQueue(event_list)\n",
    "\n",
    "    # Simulation to be wrapped\n",
    "    return acnsim.Simulator(\n",
    "        deepcopy(cn),\n",
    "        algorithm,\n",
    "        deepcopy(event_queue),\n",
    "        start,\n",
    "        period=period,\n",
    "        verbose=False,\n",
    "        interface_type=interface_type,\n",
    "    )\n",
    "\n",
    "\n",
    "def interface_generating_function(iface_type=GymTrainingInterface) -> Interface:\n",
    "    \"\"\"\n",
    "    Initializes a simulation with random events on a 1 phase, 1\n",
    "    constraint ACN (simple_acn), with 1 EVSE\n",
    "    \"\"\"\n",
    "    schedule_rl = None\n",
    "    # Simulation to be wrapped\n",
    "    sim = _random_sim_builder(schedule_rl, interface_type=iface_type)\n",
    "    iface = iface_type(sim)\n",
    "    return iface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACN-Sim gym environments wrap an interface to an ACN-Sim Simulation. These environments allow for customizable observations, reward functions, and actions through the CustomSimEnv class, and for rebuilding through the RebuildingSimEnv class (the RebuildingSimEnv class extends the CustomSimEnv class, and so has all the customizability of the latter). As an example, let's make a rebuilding simulation environment with the following characteristics:\n",
    "\n",
    "- Observations:\n",
    "    - Arrival times of all currently plugged-in EVs.\n",
    "    - Departure times of all currently plugged-in EVs.\n",
    "    - Remaining demand of all currently plugged-in EVs.\n",
    "    - Constraint matrix of the network.\n",
    "    - Limiting constraint magnitudes of the network.\n",
    "    - Current timestep of the simulation\n",
    "- Action:\n",
    "    - A zero-centered array of pilot signals. A 0 entry in the array corresponds to a charging rate of 16 A.\n",
    "- Rewards:\n",
    "    - A negative reward for each amp of violation of individual EVSE constraints.\n",
    "    - A negative reward for each amp of pilot signal delivered to an EVSE with no EV plugged in.\n",
    "    - A negative reward for each amp of network constraint violation.\n",
    "    - A positive charging reward for each amp of charge delivered if the above penalties are all 0.\n",
    "\n",
    "The observations, actions, and rewards listed here are all already encoded in the `gym_acnsim` package; see the package documentation for more details. Broadly, each observation object has space and observation generating functions. Each action is an object with space and schedule generating functions. Each reward is a function of the environment, outputting a number. The environment described here is generated by the make_rebuilding_default_env function from the gym_acnsim object; see the code there for more details. The `gym_acnsim` package provides `'default-rebuilding-acnsim-v0'`, a registered gym environment that provides this functionality. To make this environment, we need to input as a `kwarg` the `sim_gen_func` we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec_env = DummyVecEnv(\n",
    "    [\n",
    "        lambda: FlattenObservation(\n",
    "            gym.make(\n",
    "                \"default-rebuilding-acnsim-v0\",\n",
    "                interface_generating_function=interface_generating_function,\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "model = PPO2(\"MlpPolicy\", vec_env, verbose=2)\n",
    "num_iterations: int = int(1e4)\n",
    "model_name: str = f\"PPO2_{num_iterations}_test_{'default_rebuilding'}.zip\"\n",
    "model.learn(num_iterations)\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained the above model for 10000 iterations. Packaged with this\n",
    "library is the same model trained for 1000000 iterations, which we\n",
    "will now load\n",
    "model.load(model_name)\n",
    "\n",
    "\n",
    "This is a stable_baselines PPO2 model. PPO2 requires vectorized\n",
    "environments to run, so the model wrapper should convert between\n",
    "vectorized and non-vectorized environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableBaselinesRLModel(SimRLModelWrapper):\n",
    "    \"\"\" An RL model wrapper that wraps stable_baselines style models.\n",
    "    \"\"\"\n",
    "\n",
    "    model: BaseRLModel\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        observation: object,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "        info: Dict[Any, Any] = None,\n",
    "        **kwargs,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\" See SimRLModelWrapper.predict(). \"\"\"\n",
    "        return self.model.predict(observation, **kwargs)\n",
    "\n",
    "\n",
    "class GymTrainedAlgorithmVectorized(BaseAlgorithm):\n",
    "    \"\"\" Abstract algorithm class for Simulations using a\n",
    "    reinforcement learning agent that operates in an Open AI Gym\n",
    "    environment that is vectorized via stable-baselines VecEnv style\n",
    "    constructions.\n",
    "\n",
    "    Implements abstract class BaseAlgorithm.\n",
    "\n",
    "    Vectorized environments in stable-baselines do not inherit from\n",
    "    gym Env, so we must define a new algorithm class that handles\n",
    "    models that use these environments.\n",
    "\n",
    "    Args:\n",
    "        max_recompute (int): See BaseAlgorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    _env: Optional[DummyVecEnv]\n",
    "    max_recompute: Optional[int]\n",
    "    _model: Optional[SimRLModelWrapper]\n",
    "\n",
    "    def __init__(self, max_recompute: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self._env = None\n",
    "        self.max_recompute = max_recompute\n",
    "        self._model = None\n",
    "\n",
    "    def __deepcopy__(\n",
    "        self, memodict: Optional[Dict] = None\n",
    "    ) -> \"GymTrainedAlgorithmVectorized\":\n",
    "        return type(self)(max_recompute=self.max_recompute)\n",
    "\n",
    "    def register_interface(self, interface: Interface) -> None:\n",
    "        \"\"\" NOTE: Registering an interface sets the environment's\n",
    "        interface to GymTrainedInterface.\n",
    "        \"\"\"\n",
    "        if not isinstance(interface, GymTrainedInterface):\n",
    "            gym_interface: GymTrainedInterface = GymTrainedInterface.from_interface(\n",
    "                interface\n",
    "            )\n",
    "        else:\n",
    "            gym_interface: GymTrainedInterface = interface\n",
    "        super().register_interface(gym_interface)\n",
    "        if self._env is not None:\n",
    "            self.env.interface = interface\n",
    "\n",
    "    @property\n",
    "    def env(self) -> DummyVecEnv:\n",
    "        \"\"\" Return the algorithm's gym environment.\n",
    "\n",
    "        Returns:\n",
    "            DummyVecEnv: A gym environment that wraps a simulation.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Exception raised if vec_env is accessed prior to\n",
    "                an vec_env being registered.\n",
    "        \"\"\"\n",
    "        if self._env is not None:\n",
    "            return self._env\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"No vec_env has been registered yet. Please call \"\n",
    "                \"register_env with an appropriate environment before \"\n",
    "                \"attempting to call vec_env or schedule.\"\n",
    "            )\n",
    "\n",
    "    def register_env(self, env: DummyVecEnv) -> None:\n",
    "        \"\"\" Register a model that outputs schedules for the simulation.\n",
    "\n",
    "        Args:\n",
    "            env (DummyVecEnv): An vec_env wrapping a simulation.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self._env = env\n",
    "\n",
    "    @property\n",
    "    def model(self) -> SimRLModelWrapper:\n",
    "        \"\"\" Return the algorithm's predictive model.\n",
    "\n",
    "        Returns:\n",
    "            SimRLModelWrapper: A predictive model that returns an array\n",
    "                of actions given an environment wrapping a simulation.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Exception raised if model is accessed prior to\n",
    "                a model being registered.\n",
    "        \"\"\"\n",
    "        if self._model is not None:\n",
    "            return self._model\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"No model has been registered yet. Please call \"\n",
    "                \"register_model with an appropriate model before \"\n",
    "                \"attempting to call model or schedule.\"\n",
    "            )\n",
    "\n",
    "    def register_model(self, new_model: SimRLModelWrapper) -> None:\n",
    "        \"\"\" Register a model that outputs schedules for the simulation.\n",
    "\n",
    "        Args:\n",
    "            new_model (SimRLModelWrapper): A model that can be used for\n",
    "                predictions in ACN-Sim.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self._model = new_model\n",
    "\n",
    "    def schedule(self, active_evs) -> Dict[str, List[float]]:\n",
    "        \"\"\" Creates a schedule of charging rates for each EVSE in the\n",
    "        network. This only works if a model and environment have been\n",
    "        registered.\n",
    "\n",
    "        Overrides BaseAlgorithm.schedule().\n",
    "\n",
    "        The environment is assumed to be vectorized.\n",
    "        \"\"\"\n",
    "        if self._model is None or self._env is None:\n",
    "            raise TypeError(\n",
    "                f\"A model and environment must be set to call the \"\n",
    "                f\"schedule function for GymAlgorithm.\"\n",
    "            )\n",
    "        env: BaseSimEnv = self._env.envs[0].env\n",
    "        if not isinstance(env.interface, GymTrainedInterface):\n",
    "            raise TypeError(\n",
    "                \"GymAlgorithm environment must have an interface of \"\n",
    "                \"type GymTrainedInterface to call schedule(). \"\n",
    "            )\n",
    "        env.update_state()\n",
    "        env.store_previous_state()\n",
    "        env.action = self.model.predict(\n",
    "            self._env.env_method(\"observation\", env.observation)[0],\n",
    "            env.reward,\n",
    "            env.done,\n",
    "            env.info,\n",
    "        )[0]\n",
    "        env.schedule = env.action_to_schedule()\n",
    "        return env.schedule\n",
    "\n",
    "\n",
    "evaluation_algorithm = GymTrainedAlgorithmVectorized()\n",
    "evaluation_simulation = _random_sim_builder(evaluation_algorithm, GymTrainedInterface)\n",
    "\n",
    "# Make a new, single-use environment with only charging rewards.\n",
    "observation_objects: List[SimObservation] = default_observation_objects\n",
    "action_object: SimAction = default_action_object\n",
    "reward_functions: List[Callable[[BaseSimEnv], float]] = [\n",
    "    reward_functions.hard_charging_reward\n",
    "]\n",
    "eval_env: DummyVecEnv = DummyVecEnv(\n",
    "    [\n",
    "        lambda: FlattenObservation(\n",
    "            CustomSimEnv(\n",
    "                evaluation_algorithm.interface,\n",
    "                observation_objects,\n",
    "                action_object,\n",
    "                reward_functions,\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "evaluation_algorithm.register_env(eval_env)\n",
    "evaluation_algorithm.register_model(StableBaselinesRLModel(model))\n",
    "\n",
    "evaluation_simulation.run()\n",
    "\n",
    "plt.plot(acnsim.aggregate_current(evaluation_simulation))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
